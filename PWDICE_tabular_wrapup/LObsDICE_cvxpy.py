from LP_solver import Solver
import argparse
import random
import numpy as np
import torch
import copy
import time
from tqdm import tqdm
import math
import wandb
from datetime import datetime
from tabular_MDP import TabularMDP, GridWorld
from visualizer import Plotter
import subprocess

from hyperparams import ini_hpp
import time
hyperparams = ini_hpp("params/params_LobsDICE_nosample.txt")

N_expert_traj = hyperparams["N_expert_traj"]
TA_expert_traj = hyperparams["TA_expert_traj"]
grid_size = hyperparams["grid_size"]
max_step = hyperparams["max_step"]
noise_level = hyperparams["noise_level"]
TA_optimality = hyperparams["TA_optimality"]

class LobsDICE_Solver:
    def __init__(self, real_MDP, MDP, TA_dataset, time=None):
        self.MDP = MDP
        # print(self.MDP.T[self.MDP.ed, self.MDP.ed, :])
        self.MDP.T[:, self.MDP.ed, :] = 0
        self.MDP.T[self.MDP.ed, self.MDP.ed, :] = 1 # stay at the same location; effectively "absorbing state"
        
        self.MDP.T = self.MDP.T.transpose(1, 2, 0) # p(s'|s,a) -> p(s,a->s')
        self.MDP.R = -0.01 * np.ones((self.MDP.n, self.MDP.m))
        self.MDP.R[self.MDP.ed, :] = 1
        
        self.MDP.p0 = np.zeros_like(self.MDP.p0) 
        self.MDP.p0[self.MDP.st] = 1
        self.time = time if time is not None else time.time()
        self.real_MDP = real_MDP
        self.TA_dataset = TA_dataset
        self.visualizer = Plotter(int(math.sqrt(self.MDP.n)), self.MDP.st, self.MDP.ed, self.time, directory="res/LobsDICE_cvxpy/fig")
        
    def draw_policy(self, pi):
        self.visualizer.clear()
        self.visualizer.draw_grid()
        self.visualizer.draw_policy(pi)
        self.visualizer.save("LObsDICE_policy")
        
    def get_expert_policy(self):
        pi = np.zeros((self.MDP.n, self.MDP.m))
        for i in range(self.MDP.n):
            x, y = self.MDP.get_pos(i)
            if self.MDP.edy < y: pi[i, 1] = 1 # left
            elif self.MDP.edy > y: pi[i, 0] = 1 # right
            elif self.MDP.edx < x: pi[i, 3] = 1 # up
            elif self.MDP.edx > x: pi[i, 2] = 1 # down  
            else: pi[i] = np.ones(self.MDP.m) / self.MDP.m # at optimal point
        return pi
        
    def compute_marginal_distribution(self, mdp, pi, regularizer=0):
        """
        d: |S||A|
        """
        p0_s = mdp.p0
        p0 = (p0_s[:, None] * pi).reshape(mdp.n * mdp.m)
        # print(p0)
        print("T-shape:", mdp.T.shape, "checker:", mdp.T[0, 0])
        P_pi = (mdp.T.reshape(mdp.n * mdp.m, mdp.n)[:, :, None] * pi).reshape(mdp.n * mdp.m, mdp.n * mdp.m)
        
        d = np.ones(mdp.n * mdp.m); d /= np.sum(d)
        D = np.diag(d)
        E = np.sqrt(D) @ (np.eye(mdp.n * mdp.m) - mdp.gamma * P_pi)
        #print(P_pi[0], P_pi.shape)
        #exit(0)
        Q = np.linalg.solve(E.T @ E + regularizer * np.eye(mdp.n * mdp.m), (1 - mdp.gamma) * p0)
        # Q_hat = np.linalg.inv(E.T @ E + regularizer * np.eye(mdp.n * mdp.m)) @ ((1 - mdp.gamma) * p0)# for debugging
        # print("ERROR:", np.linalg.norm(Q_hat - Q)) this error is very small (6e-13). 
        w = Q - mdp.gamma * P_pi @ Q
        print("gamma:", mdp.gamma)#, P_pi @ np.ones((mdp.n * mdp.m, 1)))
        print(np.ones((1, mdp.n * mdp.m)) @ w - mdp.gamma * np.ones((1, mdp.n * mdp.m)) @ P_pi.T @ w - (1 - mdp.gamma) * mdp.n * mdp.m * np.ones((1, mdp.n * mdp.m)) @ p0, np.ones((1, mdp.n * mdp.m)) @ P_pi.T)
        # This is simply (I - \gamma P_\pi)^T y = (1-\gamma) p_0, where p_0 and y are (MDP.n * MDP.m)-dimensional vectors.
        # For positive definiteness they write like this and add regularizer. But is it necessary for a linear programming?
        # print((np.eye(mdp.n * mdp.m) - mdp.gamma * P_pi).T @ w - mdp.n * mdp.m * (1-mdp.gamma) * p0) almost 0
        print("sum:", w.sum())# P_pi.shape, np.linalg.norm((np.eye(mdp.n * mdp.m) - mdp.gamma * P_pi).T @ w - (1 - mdp.gamma) * p0))
        assert np.all(w > -1e-3), w
        d_pi = w * d
        d_pi[w < 0] = 0
        d_pi /= np.sum(d_pi)
        return d_pi
    
    def solve(self, TS_dataset, args):
        # strangely, the SMODICE author in their code assumes that they have access to the random policy besides TA-dataset generated by the random policy.
        
        # get expert and TA marginal distribution
        
        pi_b = np.zeros((self.MDP.n, self.MDP.m))
        for i in range(len(TA_dataset)):
            pi_b[TA_dataset[i]["state"], TA_dataset[i]["action"]] += 1
        for i in range(self.MDP.n):
            if pi_b[i].sum() == 0: pi_b[i] = np.ones(self.MDP.m) / self.MDP.m
            else: pi_b[i] /= pi_b[i].sum()
        print("pi_b:", pi_b)
        
        
        # pi_b = np.ones((self.MDP.n, self.MDP.m)) / self.MDP.m
        
        d = self.compute_marginal_distribution(self.MDP, pi_b)  # |S||A|
        d_s = d.reshape(self.MDP.n, self.MDP.m).sum(axis=1) # |S| (task-agnostic dataset)
        d_ss_E = np.zeros((self.MDP.n, self.MDP.n))
        N_expert_traj = 5
        
        self.mode = args.TS_type
        N = 1 / len(TS_dataset)
        rho_E = np.zeros(self.MDP.n) 
        print("mode:", self.mode)
        if self.mode == "full":
            # full expert dataset
            for i in range(len(TS_dataset)):
                # rho_E[TS_dataset[i]["state"]] += 1 / len(TS_dataset)
                rho_E[TS_dataset[i]["state"]] += (1 - self.MDP.gamma) * (self.MDP.gamma ** TS_dataset[i]["step"]) / N_expert_traj 
                
                d_ss_E[TS_dataset[i]["state"], TS_dataset[i]["next_state"]] += (1 - self.MDP.gamma) * (self.MDP.gamma ** TS_dataset[i]["step"]) / N_expert_traj 
                # print(TS_dataset[i]["next_state"], self.MDP.ed)
                if TS_dataset[i]["next_state"] == self.MDP.ed:
                    rho_E[self.MDP.ed] += self.MDP.gamma ** (TS_dataset[i]["step"] + 1) / N_expert_traj
                    d_ss_E[self.MDP.ed, self.MDP.ed] += self.MDP.gamma ** (TS_dataset[i]["step"] + 1) / N_expert_traj 
                    # print("!!", self.MDP.gamma ** (TS_dataset[i]["step"] + 1) / N_expert_traj)
        elif self.mode == "perfect_full":
            rho_E = self.compute_marginal_distribution(self.MDP, self.get_expert_policy()).reshape(self.MDP.n, self.MDP.m).sum(axis=1) # mdp_expert for mismatch
        elif self.mode == "goal":
            # goal-based
            rho_E[self.MDP.ed] = 1
        else:
            for i in range(len(TS_dataset)):
                rho_E[TS_dataset[i]["state"]] += 1 / len(TS_dataset)

        rho_E /= rho_E.sum()
        d_ss_E /= d_ss_E.sum()
        
        d_expert_s = rho_E
        d0 = d.reshape(self.MDP.n, self.MDP.m)
        d_sas_matrix = torch.zeros(self.MDP.n, self.MDP.m, self.MDP.n).to('cuda:0')
        print("d:", d.shape, "d_sas:", d_sas_matrix.shape, "T:", self.MDP.T.shape)
        for i in range(self.MDP.n):
            for j in range(self.MDP.m):
                d_sas_matrix[i, j] = d0[i, j] * torch.from_numpy(self.MDP.T[i, j]).to('cuda:0') # vector = scalar * vector
        
        d_ss_I = d_sas_matrix.sum(dim=1) # summing over action axis
        
        
        
        # train expert discriminator ...
        
        delta = 0.0001
        
        d_ss_E = torch.from_numpy(d_ss_E).to('cuda:0')
        print("d_ss_E:", d_ss_E[0], "d_ss_I:", d_ss_I[0])
        print((d_ss_E + delta).shape, (d_ss_E + d_ss_I + delta).shape)
        # C = torch.from_numpy((d_expert_s + delta) / (d_s + d_expert_s + delta))
        # C = (d_ss_E + delta) / (d_ss_E + d_ss_I + delta)
        # C(s, s') = d_E(s, s') / [d_E(s, s') + d_I(s, s')]
        
        
        
        print("d_expert_s:", d_expert_s)
        # print("C:", C[0])
        # exit(0)
        # R = -torch.log(1 / C - 1 + delta).to('cuda:0')
        R = torch.log((d_ss_E + delta) / (d_ss_I + delta))
        # print("R:", R[0], R[80])
        # exit(0)
        initials = []
        
        terminal = np.zeros(len(self.TA_dataset))
        
        for i in range(len(self.TA_dataset)):
            if self.TA_dataset[i]["step"] == 0:
                initials.append(self.TA_dataset[i]["state"])
            if i < len(self.TA_dataset) - 1:
                if self.TA_dataset[i + 1]["step"] != self.TA_dataset[i]["step"] + 1:
                    terminal[i] = 1
            else: 
                terminal[i] = 1
            # if terminal[i] == 1: print("terminal:", i)
        terminal = torch.from_numpy(terminal).to('cuda:0')
        
        
        
        alpha = 0.01
        
        # perfect policy
        d_sa_matrix = torch.from_numpy(d.reshape(self.MDP.n, self.MDP.m)).to('cuda:0')
        # calculating d_sas_matrix
        d_sas_matrix = torch.zeros(self.MDP.n, self.MDP.m, self.MDP.n).to('cuda:0')
        
        for i in range(self.MDP.n):
            for j in range(self.MDP.m):
                d_sas_matrix[i, j] = d_sa_matrix[i, j] * torch.from_numpy(self.MDP.T[i, j]).to('cuda:0') # vector = scalar * vector
        d_ss_matrix = d_sas_matrix.sum(dim=1)
        
        # cvxpy solver
        """
        import cvxpy as cp
        x = cp.Variable(self.MDP.n)
        A0 = cp.Variable((self.MDP.n, self.MDP.n))
        constraints = []
        for i in range(self.MDP.n):
            for j in range(self.MDP.n):
                constraints.append(A0[i, j] == R[i, j].detach().cpu().numpy() + self.MDP.gamma * x[j] - x[i])
        objective = cp.Minimize((1 - self.MDP.gamma) * cp.sum(cp.multiply(self.MDP.p0, x)) + (1 + alpha) * cp.log_sum_exp(np.log(1e-20 + d_ss_matrix.detach().cpu().numpy()) + A0 / (1 + alpha) - 1))
        prob = cp.Problem(objective, constraints)
        result = prob.solve()
        print("retreived V:", x.value)
        # print("A:", A0.value)
        
        exit(0)
        """
        import cvxpy as cp
        p0 = self.MDP.p0.reshape(-1, 1)
        one = np.ones((self.MDP.n, 1))
        x = cp.Variable((self.MDP.n, 1))
        objective = cp.Minimize((1 - self.MDP.gamma) * cp.sum(cp.multiply(p0, x)) + (1 + alpha) * cp.sum(cp.multiply(d_ss_matrix.cpu().detach().numpy(), cp.exp((R.cpu().detach().numpy() + self.MDP.gamma * one @ x.T - x @ one.T) / (1 + alpha) - 1))))
        prob = cp.Problem(objective)
        result = prob.solve()
        print("solved V:", x.value)
        V = torch.from_numpy(x.value).to('cuda')
        """
        V = torch.nn.parameter.Parameter(torch.rand(self.MDP.n, requires_grad=True, device='cuda'))
        optimizer = torch.optim.Adam([V])
        
        for _ in tqdm(range(2000)):
            loss = (1 - self.MDP.gamma) * (torch.from_numpy(self.MDP.p0).to('cuda:0') * V).sum()
            s = torch.zeros(1).squeeze().to('cuda:0')
            
            for i in range(self.MDP.n):
                for j in range(self.MDP.m):
                    A = R[i] + self.MDP.gamma * V - V[i] # vector + vector - scalar # question: (torch.from_numpy(self.MDP.T[i, j, :]).to('cuda:0') factor ?
                    # print("A:", A)
                    s += (d_sas_matrix[i, j] * torch.exp(A / (1 + alpha) - 1)).sum()
                    # for k in range(self.MDP.n):
                    #    A[i, j, k] = R[i, k] + self.MDP.gamma * V[k] - V[i]
                    #    s += d_sas_matrix[i, j, k] * torch.exp(A / (1 + alpha) - 1) # d_matrix[i, j] * torch.exp(R[i] + self.MDP.gamma * (torch.from_numpy(self.MDP.T[i, j]).to('cuda:0') * V).sum() - V[i])
                    
            print(s.shape, loss.shape, s, loss)
            loss += (1 + alpha) * torch.log(s)
            optimizer.zero_grad()  
            loss.backward()
            print("V:", V)
            # wandb.log({"grad_norm": torch.norm(V.grad),"loss": loss, "V_mean": V.mean(), "V_std": V.std()})
            optimizer.step()
        print("R:", R)
        print("TS:", TS_dataset)
        print("V_cvxpy:", x.value, "V_iteration:", V.cpu().detach().numpy(), V.cpu().detach().numpy().reshape(-1) - x.value.reshape(-1))
        """
        
        """
        BS = 512

        for _ in tqdm(range(1000)):
            idx_epoch = torch.randperm(len(TA_dataset))
            for __ in range(len(TA_dataset) // BS + 1): # assume len(TA_dataset) % BS != 0
                idx_ini = np.random.randint(0, len(initials), size=BS)
                idx = idx_epoch[__ * BS: min((__ + 1) * BS, len(TA_dataset))]
                state_now, state_next, state_ini = [], [], []
                for i in range(len(idx)):
                    state_now.append(self.TA_dataset[idx[i]]["state"])
                    state_next.append(self.TA_dataset[idx[i]]["next_state"])
                state_ini = np.array(initials)[idx_ini]
                state_now, state_next = np.array(state_now), np.array(state_next)
                # reward = R[state_now]
                reward = torch.zeros_like(torch.from_numpy(state_now)).to('cuda:0')
                for i in range(state_now.shape[0]):
                    reward[i] = R[state_now[i], state_next[i]]
                # print("reward:", reward.shape, "V[N]:", V[state_next].shape, "V[o]:", V[state_now].shape)
                
                A = (reward + self.MDP.gamma * V[state_next] - V[state_now]) 
                B = R[self.MDP.ed, self.MDP.ed] + self.MDP.gamma * V[self.MDP.ed] - V[self.MDP.ed]
                loss = (1 - self.MDP.gamma) * V[state_ini].mean() + (1 + alpha) * torch.log(torch.exp(A / (1 + alpha) - 1).mean() * (1 - d_s[self.MDP.ed]) + torch.exp(B / (1 + alpha) - 1) * d_s[self.MDP.ed])
                optimizer.zero_grad()  
                loss.backward()
                wandb.log({"grad_norm": torch.norm(V.grad), "loss": loss, "V_mean": V.mean(), "V_std": V.std()})
                optimizer.step()
        """
        """
        # mu = 1 / (1 + alpha) * (-alpha * R + self.MDP.gamma * V.view(1, -1) - V.view(-1, 1))
        # mu, w, e = torch.zeros((self.MDP.n, self.MDP.n)).to('cuda:0'), torch.zeros((self.MDP.n, self.MDP.m)).to('cuda:0'), torch.zeros((self.MDP.n, self.MDP.m)).to('cuda:0')
        
        for i in range(self.MDP.n):
            for j in range(self.MDP.n):
                mu[i, j] = 1 / (1 + alpha) * (-alpha * R[i, j] + self.MDP.gamma * V[j] - V[i])
        """
        w = np.zeros((self.MDP.n, self.MDP.m))
        self.pi = np.zeros((self.MDP.n, self.MDP.m))
        for i in range(self.MDP.n):
            for j in range(self.MDP.m):
                A = R[i] + self.MDP.gamma * V - V[i]
                w[i, j] = np.sum(self.MDP.T[i, j] * torch.exp(A / (1 + alpha)).cpu().detach().numpy())
            self.pi[i] = w[i] * d.reshape(self.MDP.n, self.MDP.m)[i]
            if self.pi[i].sum() < 1e-12: self.pi[i] = np.ones(self.MDP.m) / self.MDP.m
            else: self.pi[i] /= self.pi[i].sum()
        
        """
        for i in range(self.MDP.n):
            for j in range(self.MDP.m):
                e[i, j] = self.MDP.gamma * (torch.from_numpy(self.MDP.T[i, j]).to('cuda:0') * V).sum() - V[i]
                # w[i, j] = torch.exp(e[i, j] / alpha - 1)
        self.pi = torch.nn.Softmax(dim=1)(e / alpha).cpu().detach().numpy()
        """
        # w = w.detach().cpu().numpy()
       
        #self.pi = d.reshape(self.MDP.n, self.MDP.m) * w / (d.reshape(self.MDP.n, self.MDP.m) * w).sum(axis=1).reshape(-1, 1)        
        
        # 1 / alpha is too big
        """
        self.pi = np.zeros((self.MDP.n, self.MDP.m))
        for i in range(self.MDP.n):
            if i != self.MDP.ed: self.pi[i, e[i].argmax()] = 1
        """
        print("self.pi:", self.pi)
        print("w:", w)
        print("d:", d)
        self.draw_policy(self.pi)
        
    def evaluation(self, eval_use_argmax):
        T = 10
        self.visualizer.clear()
        self.visualizer.draw_grid()
        avg_r, avg_suc, tot_l = 0, 0, 0    
        for i in range(T):
            agent_buffer, r, l = self.real_MDP.evaluation(self.pi, log=True, return_reward=True, collect=True, deterministic=(eval_use_argmax == "yes")) 
            avg_r += r / T
            if r > 0: 
                avg_suc += 1
                tot_l += l 
            self.visualizer.draw_traj(agent_buffer, "orange")
        self.visualizer.save("agent_traj")
        return avg_r, avg_suc / T, 999999 if avg_suc == 0 else tot_l / avg_suc


def get_args():
    parser =  argparse.ArgumentParser()
    parser.add_argument("--seed", help="seed",type=int, default=1234567)
    parser.add_argument("--data_index", help="data_index", type=int, default=0)
    parser.add_argument("--TS_type", type=str, default="full") # "full" or "goal"
    args = parser.parse_args()
    return args

def get_git_diff():
    tmp = subprocess.run(['git', 'diff', '--exit-code'], capture_output=True)
    tmp2 = subprocess.run(['git', 'diff', '--cached', '--exit-code'], capture_output=True)
    return tmp.stdout.decode('ascii').strip() + tmp2.stdout.decode('ascii').strip()
    
def git_commit(runtime):
    tmp = subprocess.run(['git', 'commit', '-a', '-m', runtime], capture_output=True)
    return tmp.stdout.decode('ascii').strip()

if __name__ == "__main__":
    args = get_args()
    TS_type = args.TS_type
    assert TS_type == "full", "LobsDICE cannot do goal!"
    seed = args.seed
    runtime = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
    if len(get_git_diff()) > 0:
        git_commit(runtime)

    torch.manual_seed(seed) 
    torch.cuda.manual_seed_all(seed) # when using multiple GPUs torch.cuda.manual_seed(seed)
    np.random.seed(seed) 
    random.seed(seed) 
    torch.backends.cudnn.deterministic = True 
    torch.backends.cudnn.benchmark = False # CUDNN will try different methods and use an optimal one if this is set to true. This could be harmful if your input size / architecture is changing. 
    
    MDP = GridWorld(grid_size, 0, 0, grid_size - 1, grid_size - 1, noise=noise_level, max_step=max_step)
    # MDP = GridWorld(2, 0, 0, 1, 1)
    TS_dataset = torch.load("data/"+str(N_expert_traj)+"_"+str(TA_expert_traj)+"_"+str(grid_size)+"_"+str(noise_level)+"_"+str(max_step)+"_"+str(TA_optimality)+"/"+str(args.data_index)+"/TS.pt") # MDP.generate_expert_traj(N_expert_traj)
    
    TA_dataset = torch.load("data/"+str(N_expert_traj)+"_"+str(TA_expert_traj)+"_"+str(grid_size)+"_"+str(noise_level)+"_"+str(max_step)+"_"+str(TA_optimality)+"/"+str(args.data_index)+"/TA.pt") # MDP.generate_random_traj(TA_expert_traj) # 1000 traj * 25 steps / traj (s,a,s') a list of length 25000
    
    MDP_estimate = copy.deepcopy(MDP)

    MDP_estimate.T = np.zeros((MDP_estimate.n, MDP_estimate.n, 4)) # (s' | s, a)
    
    for i in range(len(TA_dataset)):
        MDP_estimate.T[TA_dataset[i]["next_state"], TA_dataset[i]["state"], TA_dataset[i]["action"]] += 1
    s = MDP_estimate.T.sum(axis=0)
    tag = (s == 0)
    
    random_estimation = np.zeros((MDP_estimate.n, MDP_estimate.n, 4))
    dx, dy = [0, 0, 1, -1], [1, -1, 0, 0] # 0 = right, 1 = left, 2 = down, 3 = up
    for i in range(MDP_estimate.n):
        x, y = i // MDP_estimate.S, i % MDP_estimate.S
        for j in range(4):
            p = 1
            for k in range(4):
                eks, wai = x + dx[k], y + dy[k]
                i_new = eks * MDP_estimate.S + wai
                if eks >= 0 and eks < MDP_estimate.S and wai >= 0 and wai < MDP_estimate.S:
                    p -= 0.25
                    random_estimation[i_new, i, j] = 0.25
            random_estimation[i, i, j] = p 
        
    MDP_estimate.T = random_estimation * tag + np.nan_to_num(MDP_estimate.T / s.reshape([1]+list(s.shape))) * (1 - tag) 
    
    #MDP_estimate.T = np.nan_to_num(np.ones_like(MDP_estimate.T) / MDP_estimate.n) * tag + np.nan_to_num(MDP_estimate.T / s.reshape([1]+list(s.shape))) * (1 - tag)
    """
    print("MDP_estimate transition:")
    for i in range(MDP_estimate.n):
        for j in range(MDP_estimate.m):
            for k in range(MDP_estimate.n):
               if MDP_estimate.T[k, i, j] > 0:
                   print("state:", i, "action:", j, "transition to", k, ":", MDP_estimate.T[k, i, j])
    """
    """
    for i in range(MDP.n):
        for j in range(MDP.m):
            if MDP_estimate[:, i, j] == 0:
                MDP_estimate[:, i, j] = np.ones(MDP.n) / MDP.n
            else:
                MDP_estimate[:, i, j] /= MDP_estimate[:, i, j].sum()
    """
    MDP_estimate_exact = copy.deepcopy(MDP)
    solver = LobsDICE_Solver(MDP, MDP_estimate, TA_dataset, time=runtime)
    
    solver.visualizer.clear()
    solver.visualizer.draw_grid()
    solver.visualizer.draw_traj(TS_dataset, "orange")
    solver.visualizer.save("expert_dataset")
    t0 = time.time()
    solver.solve(TS_dataset, args)
    t1 = time.time()
    
    # print("V_star:", V_star, "f_div:", f_div)
    avg_r, suc_rate, avg_len = solver.evaluation(eval_use_argmax="yes")
    print("avg_rew:", avg_r, "suc_rate:", suc_rate, "avg_len:", avg_len, "runtime:", t1 - t0)
    
    f = open("res/LobsDICE_cvxpy/"+runtime.replace("/", "-").replace(" ", "_")+"aka"+str(time.time())+".txt", "w")
        
    hyperparams["TS_type"] = args.TS_type
    
    for key in hyperparams.keys():
        f.write(key+" "+str(hyperparams[key])+"\n")
    f.write(str(avg_r)+" "+str(suc_rate)+" "+str(avg_len)+" "+str(t1 - t0)+"\n")
    
    print("avg_rew:", avg_r, "suc_rate:", suc_rate, "avg_len:", avg_len, "runtime:", t1 - t0)

    avg_r, suc_rate, avg_len = solver.evaluation(eval_use_argmax="no")
    f.write(str(avg_r)+" "+str(suc_rate)+" "+str(avg_len)+" "+str(t1 - t0)+"\n")    
    f.close()